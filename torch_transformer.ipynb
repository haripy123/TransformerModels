{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277964dd-ef7c-4a0d-bdf3-8f9ba33aa643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c22d896-7a76-436d-8ba9-86cff33cfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncod(nn.Module):\n",
    "    def __init__(self,d_model,seq_len,dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        pos_enc=torch.zeros((seq_len,d_model))\n",
    "        pos=torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
    "        div=torch.exp(torch.arange(0,d_model,2).float()*(-torch.log(torch.tensor(10000.0))/d_model))\n",
    "        pos_enc[:,0::2]=torch.sin(pos*div)\n",
    "        pos_enc[:,1::2]=torch.cos(pos*div)\n",
    "        pos_enc=pos_enc.unsqueeze(0)\n",
    "        self.register_buffer('pos_enc',pos_enc)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(x+self.pos_enc[:,x.shape[1]-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da20d831-f47b-4fc7-8fc0-2137d27c3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self,d_model:int,h:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.h=h\n",
    "        assert d_model%h==0,'emd_dim is not divisible by heads'\n",
    "        self.d_key=self.d_model//self.h\n",
    "        self.q=nn.Linear(d_model,d_model)\n",
    "        self.k=nn.Linear(d_model,d_model)\n",
    "        self.v=nn.Linear(d_model,d_model)\n",
    "        self.o=nn.Linear(d_model,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    @staticmethod\n",
    "    def compute_attention(q,k,v,dropout:nn.Dropout,mask):\n",
    "        d_key=k.shape[-1]\n",
    "        a_scores=(q@k.transpose(-1,-2))/d_key**0.5\n",
    "        if mask is not None:\n",
    "            a_scores=a_scores.masked_fill_(mask==0,-1e9)\n",
    "        a_scores=a_scores.softmax(-1)\n",
    "        if dropout is not None:\n",
    "            a_scores=dropout(a_scores)\n",
    "        return (a_scores@v),a_scores\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        query=self.q(q)\n",
    "        key=self.k(k)\n",
    "        value=self.v(v)\n",
    "        query=query.reshape(query.shape[0],-1,self.h,self.d_key).transpose(1,2)\n",
    "        key=key.reshape(key.shape[0],-1,self.h,self.d_key).transpose(1,2)\n",
    "        value=value.reshape(value.shape[0],-1,self.h,self.d_key).transpose(1,2)\n",
    "        x,self.attention_scores=MHA.compute_attention(query,key,value,self.dropout,mask)\n",
    "        x=x.transpose(1,2).reshape(x.shape[0],-1,self.h*self.d_key)\n",
    "        return self.o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf3a8e0-9e12-416f-948e-9c5fb3380a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self,d_model: int,dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=nn.LayerNorm(d_model)\n",
    "    def forward(self,x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1db905-9eb5-45f8-86b3-1bc352d7abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self,d_model,dropout):\n",
    "        super().__init__()\n",
    "        self.l1=nn.Linear(d_model,2*d_model)\n",
    "        self.l2=nn.Linear(2*d_model,d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        x=self.dropout(self.l1(x))\n",
    "        return self.dropout(self.l2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df9ce6f-84a0-4b28-a90e-0dcde9a72b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,dropout):\n",
    "        super().__init__()\n",
    "        self.mha=MHA(d_model,h,dropout)\n",
    "        self.ffb=FeedForwardBlock(d_model,dropout)\n",
    "        self.rc=nn.ModuleList([ResidualConnection(d_model,dropout) for _ in range(2)])\n",
    "    def forward(self,x,src_mask=None):\n",
    "        x=self.rc[0](x,lambda x:self.mha(x,x,x,src_mask))\n",
    "        x=self.rc[1](x,self.ffb)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297f5c7e-f2c0-4056-a87c-0892690347b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,num_blocks,d_model,h,dropout):\n",
    "        super().__init__()\n",
    "        self.layers=nn.ModuleList([EncoderBlock(d_model,h,dropout) for _ in range(num_blocks)])\n",
    "        self.norm=nn.LayerNorm(d_model)\n",
    "    def forward(self,x,mask=None):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1427f944-0c26-4604-9109-84befb3c96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,dropout):\n",
    "        super().__init__()\n",
    "        self.smha=MHA(d_model,h,dropout)\n",
    "        self.cmha=MHA(d_model,h,dropout)\n",
    "        self.ffb=FeedForwardBlock(d_model,dropout)\n",
    "        self.rc=nn.ModuleList([ResidualConnection(d_model,dropout) for _ in range(3)])\n",
    "    def forward(self,x,encoder_output,src_mask,tar_mask):\n",
    "        x=self.rc[0](x,lambda x:self.smha(x,x,x,tar_mask))\n",
    "        x=self.rc[1](x,lambda x:self.cmha(x,encoder_output,encoder_output,src_mask))\n",
    "        x=self.rc[2](x,self.ffb)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4093a094-9676-4c52-b09e-e8dd869cfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_blocks,d_model,h,dropout):\n",
    "        super().__init__()\n",
    "        self.layers=nn.ModuleList([DecoderBlock(d_model,h,dropout) for _ in range(num_blocks)])\n",
    "        self.norm=nn.LayerNorm(d_model)\n",
    "    def forward(self,x,encoder_output,src_mask,tar_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,encoder_output,src_mask,tar_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe15cdbe-3141-4748-a3e0-3f9ecff054ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,num_blocks,src_len,tar_len,vocab_size_src,vocab_size_tar,d_model,h,dropout):\n",
    "        super().__init__()\n",
    "        self.src_emb=nn.Embedding(vocab_size_src,d_model)\n",
    "        self.tar_emb=nn.Embedding(vocab_size_tar,d_model)\n",
    "        self.src_pos=PositionEncod(d_model,src_len,dropout)\n",
    "        self.tar_pos=PositionEncod(d_model,tar_len,dropout)\n",
    "        self.encoder=Encoder(num_blocks,d_model,h,dropout)\n",
    "        self.decoder=Decoder(num_blocks,d_model,h,dropout)\n",
    "        self.linear=nn.Linear(d_model,vocab_size_tar)\n",
    "    def encode(self,x,src_mask):\n",
    "        x=self.src_pos(self.src_emb(x))\n",
    "        x=self.encoder(x,src_mask)\n",
    "        return x\n",
    "    def decode(self,x,encoder_output,src_mask,tar_mask):\n",
    "        x=self.tar_pos(self.tar_emb(x))\n",
    "        x=self.decoder(x,encoder_output,src_mask,tar_mask)\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "        e_input,d_input,e_mask,d_mask,a_mask=x['encoder_input'].int(),x['decoder_input'].int(),x['encoder_mask'],x['decoder_mask'],x['attention_mask']\n",
    "        x=self.encode(e_input,e_mask)\n",
    "        x=self.decode(d_input,x,d_mask,a_mask)\n",
    "        x=self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fcbc5a9-3194-4e91-9226-a1d5cba1f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45cfbed6-303a-474d-8f3b-67d7d660b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_dataset('Helsinki-NLP/tatoeba_mt','eng-tel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8240e286-47eb-4e04-9cc3-c19c673bb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(ds,lang='eng'):\n",
    "    if lang=='eng':\n",
    "        for i in ds['test']['sourceString']:\n",
    "            yield i\n",
    "    else:\n",
    "        for i in ds['test']['targetString']:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6642874f-a7ab-4adf-9ef2-e8383d9377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_build_tokenizer(ds,lang):\n",
    "    tokenizer=Tokenizer(BPE(unk_token='[UNK]'))\n",
    "    tokenizer.pre_tokenizer=Whitespace()\n",
    "    trainer=BpeTrainer(special_tokens=['[UNK]','[PAD]','[SOS]','[EOS]'],min_frequency=1)\n",
    "    tokenizer.train_from_iterator(get_sentences(ds,lang),trainer=trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "defc7b16-83ff-447b-ab53-ef9237c9a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src=get_build_tokenizer(ds,'eng')\n",
    "tokenizer_tar=get_build_tokenizer(ds,'tel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bc99cea-2e23-4c07-b74a-4f9646a6b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLingualDataset(Dataset):\n",
    "    def __init__(self,ds,token_src,token_tar,seq_len,post_pr):\n",
    "        super().__init__()\n",
    "        self.token_src=token_src\n",
    "        self.token_tar=token_tar\n",
    "        self.seq_len=seq_len\n",
    "        self.ds=ds\n",
    "\n",
    "        self.pad_id=self.token_tar.token_to_id('[PAD]')\n",
    "        self.sos=self.token_src.token_to_id('[SOS]')\n",
    "        self.eos=self.token_src.token_to_id('[EOS]')\n",
    "        self.token_src.enable_padding(pad_id=self.token_src.token_to_id('[PAD]'),pad_token='[PAD]',length=seq_len)\n",
    "        self.token_tar.enable_padding(pad_id=self.token_tar.token_to_id('[PAD]'),pad_token='[PAD]',length=seq_len+1)\n",
    "        self.token_src.post_processor=post_pr\n",
    "        self.token_tar.post_processor=post_pr\n",
    "    def __len__(self):\n",
    "        return len(ds['test'])\n",
    "    def __getitem__(self,idx):\n",
    "        sample_src=ds['test']['sourceString'][idx]\n",
    "        sample_tar=ds['test']['targetString'][idx]\n",
    "        s_src=torch.tensor(self.token_src.encode(sample_src).ids,dtype=torch.float)\n",
    "        t_src=self.token_tar.encode(sample_tar).ids\n",
    "        t_src.remove(self.sos)\n",
    "        label=torch.tensor(t_src,dtype=torch.float)\n",
    "        t_src.remove(self.eos)\n",
    "        t_src=torch.tensor([self.sos]+t_src,dtype=torch.float)\n",
    "        encoder_mask=torch.tensor(self.token_src.encode(sample_src).attention_mask).unsqueeze(0).unsqueeze(0)\n",
    "        decoder_mask=torch.tensor(self.token_tar.encode(sample_tar).attention_mask)\n",
    "        eos_idx=decoder_mask.nonzero().max().item()\n",
    "        decoder_mask=torch.concat([decoder_mask[:eos_idx],decoder_mask[eos_idx+1:]]).unsqueeze(0).unsqueeze(0)\n",
    "        self_att_mask=torch.ones((self.seq_len,self.seq_len)).tril().int()&(decoder_mask)\n",
    "        return {'encoder_input':s_src,'decoder_input':t_src,'label':label,\n",
    "                'encoder_mask':encoder_mask,'decoder_mask':decoder_mask,'attention_mask':self_att_mask}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd75c1b2-63df-41bf-8ae1-4fe70711c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor=TemplateProcessing(\n",
    "    single='[SOS] $A [EOS]',\n",
    "    special_tokens=[\n",
    "        ('[SOS]',tokenizer_src.token_to_id('[SOS]')),\n",
    "        ('[EOS]',tokenizer_tar.token_to_id('[EOS]')),\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b9ead75-4cb8-40a8-bf34-72b7eb5f9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=22\n",
    "torch_ds=BiLingualDataset(ds,tokenizer_src,tokenizer_tar,seq_len,processor)\n",
    "train_ds=DataLoader(torch_ds,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8daa070a-8e13-44b9-b60e-95331006016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size=tokenizer_src.get_vocab_size()\n",
    "t_size=tokenizer_tar.get_vocab_size()\n",
    "s_len=t_len=22\n",
    "num_blocks,d_model,h=4,256,8\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1865d14a-a812-482b-b51f-4a7d8ed59101",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Transformer(num_blocks,s_len,t_len,s_size,t_size,d_model,h,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99b9f07d-d10d-43f8-bb8f-06d6f4f30fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optim,loss_fn,data,epochs):\n",
    "    t_loss=[]\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for idx,input in enumerate(data):\n",
    "            optim.zero_grad()\n",
    "            logits=model(input)\n",
    "            target=input['label'].long()\n",
    "            b_size,seq_len=input['encoder_input'].shape[0],input['encoder_input'].shape[1]\n",
    "            preds=logits.view(b_size,-1,seq_len)\n",
    "            loss=loss_fn(preds,target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            t_loss.append(loss.item())\n",
    "        print('Epoch:',epoch,'loss:',round(sum(t_loss)/len(t_loss),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5da98ef6-c842-42b0-8a96-f94b9394d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67fb45-de02-4115-b7b4-34015f0068f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 5.2385\n",
      "Epoch: 1 loss: 3.9777\n",
      "Epoch: 2 loss: 3.1959\n",
      "Epoch: 3 loss: 2.7639\n",
      "Epoch: 4 loss: 2.5024\n",
      "Epoch: 5 loss: 2.3199\n"
     ]
    }
   ],
   "source": [
    "train_model(model,optimizer,loss_fn,train_ds,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf641fd-0e47-4525-9acf-73d0a65d807e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
